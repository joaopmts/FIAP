{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg5ZPMO1jo4L"
      },
      "source": [
        "#**Trabalho final - Simulador de ChatBot com GenAI e VectorDB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXvR5XhTOG0O"
      },
      "source": [
        "**Nota de aten√ß√£o:**\n",
        "- Leia com aten√ß√£o o descritivo do trabalho e as orienta√ß√µes do template.\n",
        "- O trabalho deve ser entregue **respeitando a estrutura do arquivo de template**, utilizando o notebook \"Template Trabalho final - Simulador de ChatBot.ipynb\" e compactado no formato .zip.\n",
        "- Deve haver apenas um arquivo no formato .ipynb, consolidando todo o trabalho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "**Participantes (RM - NOME):**<br>\n",
        "xxxx - xxxxx<br>\n",
        "xxxx - xxxxx<br>\n",
        "xxxx - xxxxx<br>\n",
        "xxxx - xxxxx<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJpH4UFYToR"
      },
      "source": [
        "###**Caso de uso - Marketplace de classificados ve√≠culos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN3KUUndYagl"
      },
      "source": [
        "Imagine que voc√™ trabalha na empresa **iAutos** que tem como produto principal um marketplace de classificados de ve√≠culos e voc√™ como um Engenheiro de Dados, tem a miss√£o de ajudar a empresa a oferecer um melhor servi√ßo para seus clientes (vendedores e compradores)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GntkxRZeXqjv"
      },
      "source": [
        "Contexto:\n",
        "- Sua empresa oferece um servi√ßo (site/plataforma) de classificados de ve√≠culos (semelhantes aos marketplaces convencionais, mas focado em vendas de ve√≠culos), onde os vendedores (sellers) podem cadastrar e anunciar seus ve√≠culos, e compradores (buyers) podem buscar ve√≠culos de seu interesse e contatar os vendedores para negociar os ve√≠culos de seu interesse.\n",
        "- Diariamente a empresa recebe muitos chamados de d√∫vidas sobre as regras de publica√ß√£o de ve√≠culos e regras de uso do produto.\n",
        "- Veja mais detalhes das regras do documento de [Quem Somos e Pol√≠ticas de Uso](https://drive.google.com/file/d/1-ZpUOl8OA4lxa8CJ6auT42hSxaF3jclk) (em PDF).\n",
        "\n",
        "---\n",
        "Como podemos ajudar a empresa?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIOKUZecY1N2"
      },
      "source": [
        "###**Desafio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KiJ9c6VTP_R"
      },
      "source": [
        "A ideia √© criar um ChatBot mais \"turbinado\" do que praticamos nas aulas.\n",
        "Esse ChatBot ser√° ser respons√°vel por atender e responder as d√∫vidas dos clientes sobre o marketplace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1p5BJoZsObI"
      },
      "source": [
        "Criem um **ChatBot demonstrativo** usando IA Generativa para criar um novo **canal de atendimento** para tirar d√∫vidas dos clientes referente a plataforma, pol√≠ticas de uso e publica√ß√£o, para isso considere as orienta√ß√µes abaixo:\n",
        "\n",
        "- Utilizem o framework do **LangChain** para criar a l√≥gica do ChatBot, gerenciar a conex√£o com o modelo LLM e modelo de Embedding, para gerenciar o ChromaDB com base no contexto e para gerenciar a mem√≥ria do agente.\n",
        "- Usem como contexto o arquivo PDF [Quem Somos e Pol√≠ticas de Uso](https://drive.google.com/file/d/1-ZpUOl8OA4lxa8CJ6auT42hSxaF3jclk).\n",
        "- Criem o VectorDB com o ChromaDB e com base no contexto do PDF.\n",
        "  - Obs.: N√£o necessariamente precisa carregar o PDF, fiquem a vontade para definirem a melhor estrat√©gia.\n",
        "- Utilizem as boas pr√°ticas de Prompt Engineering para criar o template do prompt para o servi√ßo e gerenciar a conversa.\n",
        "- Utilizem como base e refer√™ncia todos os materiais apresentados, tanto de Generative AI quanto de Database for GenAI.\n",
        "  - Estruturem bem o trabalho, organizem em fun√ß√µes, expliquem e documentem bem os c√≥digos e decis√µes.\n",
        "- Fiquem a vontade de complementar o contexto e o prompt para otimizar o servi√ßo.\n",
        "- Fiquem a vontade de trazer t√©cnicas que fa√ßam sentido e complemente o trabalho.\n",
        "\n",
        "### Dicas:\n",
        "- Comecem os desenvolvimentos de forma simples, testando os componentes e v√° aumentando a complexidade gradativamente.\n",
        "- Utilizem os modelos LLMs da OpenAI ou da Azure com OpenAI.\n",
        "\n",
        "###**Orienta√ß√µes:**\n",
        "\n",
        "---\n",
        "####**Usem o Google Colab com Python e esse template para desenvolverem o trabalho.**\n",
        "---\n",
        "\n",
        "**1. Desenvolvimento e testes**, nessa parte √© onde voc√™s podem explorar o desenvolvimento do trabalho aplicando as t√©cnicas, testando as ferramentas e servi√ßos de GenAI.\n",
        "  - Explorem diferentes formas de tratar o problema. Comecem testando os componentes e definindo a melhor estrat√©gia.\n",
        "  - Testem as ferramentas, framework, APIs e t√©cnicas de prompt engineering.\n",
        "  - Fiquem √† vontade para explorar os servi√ßos e frameworks vistos em aula: API da OpenAI Platform, API da Azure AI Foundry, LangChain e outros.\n",
        "  - Sejam criativos, mas n√£o precisa de muita complexidade e podem explorar outras formas de desenvolver.\n",
        "  - Expliquem as decis√µes e racional do desenvolvimento. **Abuse dos coment√°rios**.\n",
        "\n",
        "**2. Processo final**, aqui nessa parte separem apenas o processo final com um pipeline completo para o ChatBot, desde a instala√ß√£o das bibliotecas at√© a simula√ß√£o.\n",
        "  - Organizem em fun√ß√µes que fa√ßam sentido.\n",
        "  - Resultado esperado: Um processo estruturado utilizando LangChain e ChromaDB, criando um VectorDB com uma boa estrat√©gia de indexa√ß√£o e busca (retriever), uma simula√ß√£o do ChatBot (pode ser estruturando uma API ou alguma l√≥gica para simular uma conversa).\n",
        "  - Exemplos: Deixem e/ou compartilhem no notebook exemplos de conversas.\n",
        "  - Testem bem esse pipeline antes, pois o professor tentar√° executar o processo para validar a implementa√ß√£o.\n",
        "\n",
        "###**Avalia√ß√£o:**\n",
        "O trabalho ser√° avaliado pelas seguintes diretrizes:\n",
        "  - Demonstra√ß√£o de conhecimento com os temas abordados em sala de aula.\n",
        "  - Utiliza√ß√£o correta dos frameworks, APIs e aplica√ß√£o das t√©cnicas de prompt engineering.\n",
        "  - Organiza√ß√£o, coment√°rios e explica√ß√£o certamente v√£o ajudar na nota.\n",
        "  - Resultado esperado seguindo as orienta√ß√µes do professor nesse template.\n",
        "\n",
        "###**Aten√ß√£o:**\n",
        "- Usem a conta da **Azure AI Foundry** ou da **OpenAI Platform** para desenvolverem o trabalho, mas d√™ prefer√™ncia para a conta da Azure por causa dos limites de cr√©dito. **N√£o deixem suas credenciais no trabalho, por favor!**\n",
        "- Trabalhos iguais s√£o pass√≠veis de reprova√ß√£o ou desconto de nota.\n",
        "- Respeitem a estrutura do template fornecido pelo professor.\n",
        "- Limite de 2 a 4 pessoas por grupo, de prefer√™ncia o mesmo grupo do Startup One."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EISVWz-KmH5D"
      },
      "source": [
        "##**1. Desenvolvimento e testes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh9AhDTBmH5D"
      },
      "source": [
        "Desenvolva aqui:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTukXNWimOuU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "KEY_NGROK = \"\" # @param {\"type\":\"string\"}\n",
        "\n",
        "os.environ[\"KEY_NGROK\"] = KEY_NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, time, psutil, socket, subprocess, shutil\n",
        "\n",
        "CHROMA_HOST = \"127.0.0.1\" \n",
        "CHROMA_PORT = 8000\n",
        "\n",
        "def is_port_in_use(port, host=\"127.0.0.1\"):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(0.5)\n",
        "        return s.connect_ex((host, port)) == 0\n",
        "\n",
        "def wait_for_port(port, host=\"127.0.0.1\", timeout=30):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        if is_port_in_use(port, host):\n",
        "            return True\n",
        "        time.sleep(0.5)\n",
        "    return False\n",
        "\n",
        "def build_chroma_cmd(host, port):\n",
        "        chroma_exe = os.path.join(os.path.dirname(sys.executable), \"Scripts\", \"chroma.exe\")\n",
        "        return [chroma_exe, \"run\", \"--host\", host, \"--port\", str(port)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "chroma_exe = os.path.join(os.path.dirname(sys.executable), \"Scripts\", \"chroma.exe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\JOAO PC\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Scripts\\\\chroma.exe'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chroma_exe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.exists(chroma_exe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_port_in_use(CHROMA_PORT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chroma = build_chroma_cmd(CHROMA_HOST, CHROMA_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['c:\\\\Users\\\\JOAO PC\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Scripts\\\\chroma.exe',\n",
              " 'run',\n",
              " '--host',\n",
              " '127.0.0.1',\n",
              " '--port',\n",
              " '8000']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['c:\\\\Users\\\\JOAO PC\\\\AppData\\\\Local\\\\Program...>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subprocess.Popen(chroma, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wait_for_port(CHROMA_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando status do servidor...\n",
            "{\"nanosecond heartbeat\":1761933964922155600}\n",
            "\n",
            "Se a resposta acima for '{\"nanosecond heartbeat\":...}', o servidor est√° no ar!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100    44  100    44    0     0  29810      0 --:--:-- --:--:-- --:--:-- 44000\n"
          ]
        }
      ],
      "source": [
        "# Verifica se o servidor est√° respondendo (opcional, mas recomendado)\n",
        "print(\"Verificando status do servidor...\")\n",
        "!curl http://127.0.0.1:8000/api/v2/heartbeat\n",
        "print(\"\\nSe a resposta acima for '{\\\"nanosecond heartbeat\\\":...}', o servidor est√° no ar!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configurando e iniciando o Ngrok\n",
            "Servidor Chroma est√° online e acess√≠vel na URL: NgrokTunnel: \"https://operculate-vernon-unmissed.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "# Associar o Servidor Chroma com o Ngrok\n",
        "import time\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Configurando e iniciando o Ngrok\n",
        "print(\"Configurando e iniciando o Ngrok\")\n",
        "ngrok.set_auth_token(KEY_NGROK)\n",
        "#conf.get_default().auth_token = KEY_NGROK\n",
        "\n",
        "# Mata qualquer t√∫nel anterior para garantir um in√≠cio limpo\n",
        "try:\n",
        "  ngrok.kill()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Expor o servidor com Ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Servidor Chroma est√° online e acess√≠vel na URL: {public_url}\")\n",
        "# Inicia o t√∫nel na porta 8000, onde nosso servidor Chroma est√° escutando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://operculate-vernon-unmissed.ngrok-free.dev'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "public_url.public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"nanosecond heartbeat\":1761933402510016300}\n",
            "\n",
            "Se a resposta acima for '{\"nanosecond heartbeat\":...}', o servidor est√° no ar! üöÄ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0    44    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100    44  100    44    0     0     62      0 --:--:-- --:--:-- --:--:--    62\n"
          ]
        }
      ],
      "source": [
        "url = public_url.public_url  \n",
        "\n",
        "# testa o endpoint de heartbeat\n",
        "!curl {url}/api/v2/heartbeat\n",
        "print(\"\\nSe a resposta acima for '{\\\"nanosecond heartbeat\\\":...}', o servidor est√° no ar! üöÄ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4NwScOmmg1c"
      },
      "source": [
        "##**2. Processo final**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "## LangChain\n",
        "!pip install langchain==0.3.27 langchain_community==0.3.27 langchain-openai==0.3.27 --quiet\n",
        "\n",
        "## ChromaDB\n",
        "!pip install langchain-chroma==0.2.5 chromadb-client==1.0.20 --quiet\n",
        "\n",
        "## Ngrok\n",
        "#!pip install pyngrok==7.3.0 --quiet\n",
        "\n",
        "## Outras\n",
        "!pip install fastapi==0.116.1 uvicorn==0.35.0 nest_asyncio==1.6.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bibliotecas e configura√ß√£o do ambiente\n",
        "import os\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import HTMLResponse\n",
        "from pydantic import BaseModel\n",
        "import uuid\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import chromadb\n",
        "\n",
        "# Bibliotecas LanChain\n",
        "import langchain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Ativa o modo debug do LangChain (para ver o que acontece por baixo do cap√¥!)\n",
        "flg_log_debug = False\n",
        "langchain.debug = flg_log_debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#**Quem Somos?**\n",
            "Bem-vindo √† AutoToya Ve√≠culos, sua concession√°ria oficial da Toyota!\n",
            "\n",
            "Desde o in√≠cio, nosso compromisso √© oferecer a voc√™ uma experi√™ncia completa e personalizada, indo al√©m da venda de ve√≠culos. Na AutoToya, voc√™ encontrar√° uma ampla variedade de modelos Toyota, desde os mais recentes lan√ßamentos at√© ve√≠culos seminovos de alta qualidade.\n",
            "\n",
            "Na AutoToya, estamos comprometidos em oferecer a voc√™ uma experi√™ncia completa e personalizada. Contamos com uma ampla variedade de modelos T\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1xZIpwP1JggimbZrtGLnuaTf6DrhbAS9T/view?usp=sharing\"\n",
        "\n",
        "file_id = url.split(\"/d/\")[1].split(\"/\")[0]\n",
        "url_download = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "file_name = \"contexto_rag_chatbot.txt\"\n",
        "\n",
        "response = requests.get(url_download)\n",
        "response.raise_for_status()\n",
        "\n",
        "with open(file_name, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "arquivo_contexto = file_name\n",
        "with open(arquivo_contexto, 'r', encoding='utf-8') as file:\n",
        "    contexto = file.read()\n",
        "\n",
        "print(contexto[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O contexto foi dividido em 12 documentos (chunks).\n"
          ]
        }
      ],
      "source": [
        "## Por quantidade de tokens\n",
        "import tiktoken\n",
        "\n",
        "# Escolhe o tokenizer utilizando o mesmo m√©todo do modelo de Embeddings\n",
        "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
        "\n",
        "# Fun√ß√£o que conta tokens\n",
        "def fn_conta_token(text: str) -> int:\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Configura o splitter medindo por tokens\n",
        "text_splitter_token = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,         # m√°ximo de tokens por chunk\n",
        "    chunk_overlap = 50,       # sobreposi√ß√£o de tokens por chunk\n",
        "    length_function = fn_conta_token\n",
        ")\n",
        "\n",
        "# O m√©todo create_documents() divide o contexto nos chunks e j√° o formata em um objeto tipo Document do LangChain.\n",
        "documents = text_splitter_token.create_documents([contexto])\n",
        "print(f\"O contexto foi dividido em {len(documents)} documentos (chunks).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JOAO PC\\AppData\\Local\\Temp\\ipykernel_4580\\114160704.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding_model = OpenAIEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "# Define qual o modelo de embeddings, no caso vamos usar um modelo da OpenAI - text-embedding-3-small\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    openai_api_key = OPENAI_API_KEY,\n",
        "    model = 'text-embedding-3-small'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "client_chromadb = chromadb.HttpClient(\n",
        "    host = CHROMA_HOST\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# O nome da \"cole√ß√£o\" no servidor. Pense nisso como o nome de uma tabela em um banco de dados.\n",
        "CHROMADB_COLLECTION_NAME = \"chromadb_vactorstore_autotoya\"\n",
        "\n",
        "# Cria o Vectorstore com o Chroma a partir dos documentos divididos\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents = documents,\n",
        "    embedding = embedding_model,\n",
        "    client = client_chromadb,  # Passamos o \"client\" em vez do persist_directory\n",
        "    collection_name = CHROMADB_COLLECTION_NAME\n",
        ")\n",
        "\n",
        "# Instancia o retriever (podemos controlar o n√∫mero de chunks do retorno da an√°lise de similaridade)\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type = \"similarity\",   # similarity por \"mmr\" - (max marginal relevance) - pode trazer mais diversidade nos resultados\n",
        "    search_kwargs = {\"k\": 3}      # define quantos chunks v√£o retornar\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#**Quem Somos?**\n",
            "Bem-vindo √† AutoToya Ve√≠culos, sua concession√°ria oficial da Toyota!\n",
            "\n",
            "Desde o in√≠cio, nosso compromisso √© oferecer a voc√™ uma experi√™ncia completa e personalizada, indo al√©m da venda de ve√≠culos. Na AutoToya, voc√™ encontrar√° uma ampla variedade de modelos Toyota, desde os mais recentes lan√ßamentos at√© ve√≠culos seminovos de alta qualidade.\n",
            "\n",
            "Na AutoToya, estamos comprometidos em oferecer a voc√™ uma experi√™ncia completa e personalizada. Contamos com uma ampla variedade de modelos Toyota, desde os mais recentes lan√ßamentos at√© ve√≠culos seminovos de alta qualidade, al√©m de pre√ßos exclusivos para taxistas, pessoas com defici√™ncia (PCD), produtores rurais e vendas diretas para CNPJ.\n",
            "\n",
            "Tamb√©m cuidamos do seu Toyota como ningu√©m. Oferecemos pe√ßas originais e servi√ßos especializados para garantir a m√°xima seguran√ßa, desempenho e durabilidade do seu ve√≠culo.\n",
            "\n",
            "Na AutoToya, acreditamos que escolher um carro √© mais do que uma compra, √© um momento especial. Venha nos visitar, conhe√ßa nossa linha completa e encontre o Toyota perfeito para voc√™!\n",
            "\n",
            "Aqui todos os ve√≠culos seminovos s√£o revisados e possuem garantia de um ano pela concession√°ria (motor e c√¢mbio).\n",
            "\n",
            "## Servi√ßos e Benef√≠cios\n",
            "- **Cons√≥rcio AutoToya:** Facilite a aquisi√ß√£o de carros, motos ou utilit√°rios, novos ou seminovos, com planos sob medida para voc√™.\n",
            "- **Pe√ßas Originais Toyota:** Garanta seguran√ßa, durabilidade e desempenho com pe√ßas genu√≠nas.\n",
            "- **Test-Drive Dispon√≠vel:** Experimente o Toyota dos seus sonhos!\n",
            "- **Garantia:** Garantia para todos os ve√≠culos e servi√ßos.\n",
            "\n",
            "## Experimente e Conhe√ßa Mais\n",
            "Venha nos visitar e aproveite para realizar um test-drive em nossos ve√≠culos.\n",
            "Nossa equipe est√° pronta para atender voc√™ com excel√™ncia e encontrar o Toyota ideal para suas necessidades.\n",
            "\n",
            "Saiba mais sobre nossos modelos, servi√ßos e ofertas em nosso site: www.autotoyaveiculos.com.br.\n",
            "Saiba mais sobre nossos modelos, servi√ßos e ofertas em nosso site: www.autotoyaveiculos.com.br.\n",
            "\n",
            "Nos destacamos com vantagens competitivas dos principais modelos da Toyota no mercado brasileiro, tornando-os escolhas populares entre os consumidores.\n",
            "\n",
            "AutoToya Toyota ‚Äì Mais do que vender carros, realizamos sonhos.\n",
            "\n",
            "\n",
            "#**Sobre a concession√°ria AutoToya Ve√≠culos:**\n",
            "\n",
            "## Hor√°rio de funcionamento:\n",
            "- Segunda a Sexta: 8h √†s 18h\n",
            "- S√°bado: 9h √†s 14h\n",
            "- Domingo: fechado\n",
            "\n",
            "## Nossas Unidades\n",
            "Visite uma de nossas lojas e descubra o Toyota perfeito para voc√™:\n",
            "- Osasco: Avenida dos Autonomistas, 890 - Centro, Osasco - SP, 06020-010\n",
            "- S√£o Paulo: Avenida Paulista, 1106 - Bela Vista, S√£o Paulo - SP, 01311-000\n",
            "- Curitiba: Avenida Sete de Setembro, 2775 - Rebou√ßas, Curitiba - PR, 80230-010\n",
            "\n",
            "## Informa√ß√µes de contato:\n",
            "- Telefone: (11) 4002-8922\n",
            "- WhatsApp: (11) 98888-7777\n",
            "- Site: www.autotoyaveiculos.com.br\n",
            "- Instagram: contato@iautoveiculos.com.br\n",
            "\n",
            "\n",
            "#**Sobre os ve√≠culos dispon√≠veis:**\n"
          ]
        }
      ],
      "source": [
        "# Teste da busca por similaridade\n",
        "query = \"O que √© a AutoToya?\"\n",
        "\n",
        "docs_retornados = vectorstore.similarity_search(query, k=2)\n",
        "print(docs_retornados[0].page_content)\n",
        "print(docs_retornados[1].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configura o chat prompt template com System Rules - regras fixas do assistente\n",
        "system_template = \"\"\"\n",
        "Voc√™ √© um assistente virtual altamente inteligente e atende clientes de uma loja (concession√°ria) de venda de ve√≠culos chamada AutoToya Ve√≠culos.\n",
        "\n",
        "#**Seu papel e objetivo √©:**\n",
        "1. Responder d√∫vidas sobre a loja, ve√≠culos e servi√ßos oferecidos pela loja.\n",
        "2. Qualificar e engajar o cliente para aumentar o interesse nos ve√≠culos (priorize os ve√≠culos zero km, mas n√£o perca a venda.)\n",
        "3. Convidar o cliente para fazer um test-drive e conhecer o ve√≠culo pessoalmente.\n",
        "4. Ajudar o cliente a agendar a visita de test-drive.\n",
        "\n",
        "##**Orienta√ß√µes e regras que deve seguir:**\n",
        "- Seu nome √© ToyaBot.\n",
        "- Responda sempre em Portugu√™s (pt-br)\n",
        "- Seja simp√°tico e prestativo. Use emojs para deixar a conversa divertida, como: ü§ñ üöó üëä üí™ üöÄ üòä ü™Ñ\n",
        "- Responda sempre de forma educada e clara.\n",
        "- Nunca destratar um cliente sendo rude ou arrogante por exemplo.\n",
        "- Use o contexto fornecido para responder √† pergunta de forma coerente.\n",
        "- **Mantenha uma coer√™ncia nas respostas com base no hist√≥rico do chat.**\n",
        "- Se o contexto n√£o for suficiente, pe√ßa mais detalhes para o cliente.\n",
        "- Responda **APENAS** perguntas que est√£o no contexto, se n√£o souber diga que n√£o pode responder (respnda de forma cordial) e interaja voltando para o contexto da conversa e seu objetivo.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_template),   ## regras fixas do assistente\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\", n_messages = 6, optional = True), ## Insere o hist√≥rico de conversas aqui (opcional e limitado a N mensagens) / Monta uma lista de mensagens (AIMessage, HumanMessage, SystemMessage)\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")        ## define a entrada do usu√°rio - entrada din√¢mica\n",
        "])\n",
        "# optional = True, deixa o hist√≥rico como opcional caso esteja vazio para evitar erros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JOAO PC\\AppData\\Local\\Temp\\ipykernel_4580\\155446263.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n"
          ]
        }
      ],
      "source": [
        "# Modelo LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.5,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JOAO PC\\AppData\\Local\\Temp\\ipykernel_4580\\2282799827.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory_test = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "# Configura o memory para manter o hist√≥rico da conversa\n",
        "memory_test = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        "    )\n",
        "\n",
        "# Cria a cadeia de conversa com retriever e mem√≥ria\n",
        "chain_test = ConversationalRetrievalChain.from_llm(\n",
        "    llm = llm,\n",
        "    retriever = retriever,\n",
        "    memory = memory_test,\n",
        "    combine_docs_chain_kwargs = {\"prompt\": chat_prompt},\n",
        "    chain_type = \"stuff\", # O tipo \"stuff\" simplesmente junta os chunks em um √∫nico prompt.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o com o loop de itera√ß√£o simulando um chat\n",
        "def fn_chat_test():\n",
        "  \"\"\"Fun√ß√£o para simular um chat com o assistente\"\"\"\n",
        "  print(\"Iniciado o Chatbot da AutoToya! Digite 'sair' para encerrar.\\n\")\n",
        "\n",
        "  # Pr√©-carregar uma mensagem de hist√≥rico com uma sauda√ß√£o inicial\n",
        "  memory_test.chat_memory.clear() ## limpa o hist√≥rico (buffer)\n",
        "  memory_test.chat_memory.add_ai_message(\"Ol√°! Seja bem-vindo. Sou um assistente virtual, me chamo ToyaBot ü§ñ. \\nComo posso ajudar voc√™ hoje? Est√° em busca de informa√ß√µes sobre algum ve√≠culo ou servi√ßo da AutoToya Ve√≠culos? üöó‚ú®\")\n",
        "  print(f\"ToyaBot ü§ñ: {memory_test.chat_memory.messages[0].content}\")\n",
        "\n",
        "  while True:\n",
        "    user_question = input(\"\\nVoc√™: \")\n",
        "    if user_question.lower() == \"sair\":\n",
        "        print(\"ToyaBot ü§ñ: üëã Conversa encerrada. At√© logo! üöó\")\n",
        "        break\n",
        "    response = chain_test.invoke({\"question\": user_question})\n",
        "    #response = chain_test.run(question = user_question) ## Essa fun√ß√£o est√° depreciada\n",
        "    print(f\"ToyaBot ü§ñ: {response['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciado o Chatbot da AutoToya! Digite 'sair' para encerrar.\n",
            "\n",
            "ToyaBot ü§ñ: Ol√°! Seja bem-vindo. Sou um assistente virtual, me chamo ToyaBot ü§ñ. \n",
            "Como posso ajudar voc√™ hoje? Est√° em busca de informa√ß√µes sobre algum ve√≠culo ou servi√ßo da AutoToya Ve√≠culos? üöó‚ú®\n",
            "ToyaBot ü§ñ: A AutoToya Ve√≠culos √© a sua concession√°ria oficial da Toyota! üöó Desde o in√≠cio, nosso compromisso √© oferecer uma experi√™ncia completa e personalizada, indo al√©m da venda de ve√≠culos. Na AutoToya, voc√™ encontrar√° uma ampla variedade de modelos Toyota, desde os mais recentes lan√ßamentos at√© ve√≠culos seminovos de alta qualidade.\n",
            "\n",
            "Estamos dedicados a cuidar do seu Toyota, oferecendo pe√ßas originais e servi√ßos especializados para garantir a seguran√ßa, desempenho e durabilidade do seu ve√≠culo. Al√©m disso, contamos com pre√ßos exclusivos para taxistas, pessoas com defici√™ncia (PCD), produtores rurais e vendas diretas para CNPJ.\n",
            "\n",
            "Na AutoToya, acreditamos que escolher um carro √© mais do que uma compra, √© um momento especial. Venha nos visitar e descubra o Toyota perfeito para voc√™! üòä\n",
            "ToyaBot ü§ñ: üëã Conversa encerrada. At√© logo! üöó\n"
          ]
        }
      ],
      "source": [
        "flg_log_debug = False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    langchain.debug = flg_log_debug\n",
        "    fn_chat_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerenciamento de m√∫ltiplos usu√°rios (mem√≥ria por sess√£o)\n",
        "sessions = {}\n",
        "\n",
        "def get_chat_chain(session_id):\n",
        "    if session_id not in sessions:\n",
        "\n",
        "        # Configura o memory para manter o hist√≥rico da conversa\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "            )\n",
        "\n",
        "        # Cria a cadeia de conversa com retriever e mem√≥ria\n",
        "        chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm = llm,\n",
        "            retriever = retriever,\n",
        "            memory = memory,\n",
        "            combine_docs_chain_kwargs = {\"prompt\": chat_prompt},\n",
        "            chain_type = \"stuff\", # O tipo \"stuff\" simplesmente junta os chunks em um √∫nico prompt.\n",
        "            )\n",
        "\n",
        "        # Carrega mensagem inicial\n",
        "        memory.chat_memory.add_ai_message(\"Ol√°! Seja bem-vindo. Sou um assistente virtual, me chamo ToyaBot ü§ñ. \\nComo posso ajudar voc√™ hoje? Est√° em busca de informa√ß√µes sobre algum ve√≠culo ou servi√ßo da AutoToya Ve√≠culos? üöó‚ú®\")\n",
        "\n",
        "        # Gerencia sess√£o - cada chat vira um objeto chain identificado pelo session_id\n",
        "        sessions[session_id] = chain\n",
        "    return sessions[session_id]\n",
        "\n",
        "# Configura o FastAPI para criar a estrutura de API\n",
        "app = FastAPI(title=\"API - Chatbot ToyaBot\", version=\"1.0\")\n",
        "\n",
        "# Estrutura resposta com parse do pydantic\n",
        "class ChatRequest(BaseModel):\n",
        "    session_id: str\n",
        "    question: str\n",
        "\n",
        "# Rota para chamar a fun√ß√£o que chama o LLM e toda estrutura do chat (chain, retriever, vectorstore, etc.)\n",
        "@app.post(\"/chat\")\n",
        "def chat(req: ChatRequest):\n",
        "    chain = get_chat_chain(req.session_id)\n",
        "    #resposta = chain.run(question = req.question)\n",
        "    resposta = chain.invoke({\"question\": req.question})\n",
        "    resposta = resposta['answer']\n",
        "    return {\"answer\": resposta}\n",
        "\n",
        "# Rota para inicializar o chat com a mensagem de sauda√ß√£o inicial\n",
        "@app.get(\"/start/{session_id}\")\n",
        "def start(session_id: str):\n",
        "    chain = get_chat_chain(session_id)\n",
        "    msg_inicial = chain.memory.chat_memory.messages[0].content\n",
        "    return {\"answer\": msg_inicial}\n",
        "\n",
        "# Rota principal que renderiza p√°gina HTML do chat (simples)\n",
        "@app.get(\"/\", response_class = HTMLResponse)\n",
        "def index():\n",
        "    return \"\"\"\n",
        "<html>\n",
        "<head><title>ü§ñ ToyaBot Chat ü§ñ</title></head>\n",
        "<body style=\"font-family:Arial\">\n",
        "  <h2>ü§ñ ToyaBot - AutoToya Ve√≠culos ü§ñ</h2>\n",
        "  <div id=\"chat\" style=\"border:1px solid #ccc; padding:10px; width:400px; height:300px; overflow-y:scroll;\"></div>\n",
        "  <input type=\"text\" id=\"msg\" style=\"width:300px;\" placeholder=\"Digite sua mensagem...\"/>\n",
        "  <button onclick=\"send()\">Enviar</button>\n",
        "  <script>\n",
        "    let session_id = Math.random().toString(36).substring(7);\n",
        "\n",
        "    async function startChat(){\n",
        "      let r = await fetch('/start/' + session_id);\n",
        "      let data = await r.json();\n",
        "      document.getElementById(\"chat\").innerHTML += \"<b>ToyaBot:</b> \" + data.answer + \"<br/>\";\n",
        "    }\n",
        "\n",
        "    async function send(){\n",
        "      let msg = document.getElementById(\"msg\").value;\n",
        "      document.getElementById(\"chat\").innerHTML += \"<b>Voc√™:</b> \" + msg + \"<br/>\";\n",
        "      let r = await fetch('/chat', {\n",
        "        method: 'POST',\n",
        "        headers: {'Content-Type':'application/json'},\n",
        "        body: JSON.stringify({session_id: session_id, question: msg})\n",
        "      });\n",
        "      let data = await r.json();\n",
        "      document.getElementById(\"chat\").innerHTML += \"<b>ToyaBot:</b> \" + data.answer + \"<br/>\";\n",
        "      document.getElementById(\"msg\").value=\"\";\n",
        "    }\n",
        "\n",
        "    // inicia o chat chamando o backend\n",
        "    startChat();\n",
        "  </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [4580]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL p√∫blica gerada pelo Ngrok: NgrokTunnel: \"https://operculate-vernon-unmissed.ngrok-free.dev\" -> \"http://localhost:8001\"\n",
            "INFO:     187.255.127.46:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     187.255.127.46:0 - \"GET /start/0lkk0q HTTP/1.1\" 200 OK\n",
            "INFO:     187.255.127.46:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     187.255.127.46:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     187.255.127.46:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [4580]\n"
          ]
        }
      ],
      "source": [
        "# Executa o App no Colab + ngrok\n",
        "if __name__ == \"__main__\":\n",
        "  #ngrok.set_auth_token(KEY_NGROK) ## J√° fizemos essa configura√ß√£o antes (agora √© opcional)\n",
        "  public_url = ngrok.connect(8001)\n",
        "  print(\"URL p√∫blica gerada pelo Ngrok:\", public_url)\n",
        "\n",
        "  nest_asyncio.apply()\n",
        "  uvicorn.run(app, host=\"0.0.0.0\", port=8001, reload=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finaliza sess√£o do Ngrok (Agente)\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9FHz-nrmg1d"
      },
      "source": [
        "Desenvolva aqui:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz_9WPHVmkvP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMnpzLAbmkst"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmdi4LFrmkhz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4kHa8kfmkfX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
