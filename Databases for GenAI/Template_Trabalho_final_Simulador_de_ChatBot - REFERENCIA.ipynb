{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg5ZPMO1jo4L"
      },
      "source": [
        "#**Trabalho final - Simulador de ChatBot com GenAI e VectorDB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXvR5XhTOG0O"
      },
      "source": [
        "**Nota de atenção:**\n",
        "- Leia com atenção o descritivo do trabalho e as orientações do template.\n",
        "- O trabalho deve ser entregue **respeitando a estrutura do arquivo de template**, utilizando o notebook \"Template Trabalho final - Simulador de ChatBot.ipynb\" e compactado no formato .zip.\n",
        "- Deve haver apenas um arquivo no formato .ipynb, consolidando todo o trabalho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "**Participantes (RM - NOME):**<br>\n",
        "xxxx - xxxxx<br>\n",
        "xxxx - xxxxx<br>\n",
        "xxxx - xxxxx<br>\n",
        "xxxx - xxxxx<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJpH4UFYToR"
      },
      "source": [
        "###**Caso de uso - Marketplace de classificados veículos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN3KUUndYagl"
      },
      "source": [
        "Imagine que você trabalha na empresa **iAutos** que tem como produto principal um marketplace de classificados de veículos e você como um Engenheiro de Dados, tem a missão de ajudar a empresa a oferecer um melhor serviço para seus clientes (vendedores e compradores)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GntkxRZeXqjv"
      },
      "source": [
        "Contexto:\n",
        "- Sua empresa oferece um serviço (site/plataforma) de classificados de veículos (semelhantes aos marketplaces convencionais, mas focado em vendas de veículos), onde os vendedores (sellers) podem cadastrar e anunciar seus veículos, e compradores (buyers) podem buscar veículos de seu interesse e contatar os vendedores para negociar os veículos de seu interesse.\n",
        "- Diariamente a empresa recebe muitos chamados de dúvidas sobre as regras de publicação de veículos e regras de uso do produto.\n",
        "- Veja mais detalhes das regras do documento de [Quem Somos e Políticas de Uso](https://drive.google.com/file/d/1-ZpUOl8OA4lxa8CJ6auT42hSxaF3jclk) (em PDF).\n",
        "\n",
        "---\n",
        "Como podemos ajudar a empresa?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIOKUZecY1N2"
      },
      "source": [
        "###**Desafio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KiJ9c6VTP_R"
      },
      "source": [
        "A ideia é criar um ChatBot mais \"turbinado\" do que praticamos nas aulas.\n",
        "Esse ChatBot será ser responsável por atender e responder as dúvidas dos clientes sobre o marketplace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1p5BJoZsObI"
      },
      "source": [
        "Criem um **ChatBot demonstrativo** usando IA Generativa para criar um novo **canal de atendimento** para tirar dúvidas dos clientes referente a plataforma, políticas de uso e publicação, para isso considere as orientações abaixo:\n",
        "\n",
        "- Utilizem o framework do **LangChain** para criar a lógica do ChatBot, gerenciar a conexão com o modelo LLM e modelo de Embedding, para gerenciar o ChromaDB com base no contexto e para gerenciar a memória do agente.\n",
        "- Usem como contexto o arquivo PDF [Quem Somos e Políticas de Uso](https://drive.google.com/file/d/1-ZpUOl8OA4lxa8CJ6auT42hSxaF3jclk).\n",
        "- Criem o VectorDB com o ChromaDB e com base no contexto do PDF.\n",
        "  - Obs.: Não necessariamente precisa carregar o PDF, fiquem a vontade para definirem a melhor estratégia.\n",
        "- Utilizem as boas práticas de Prompt Engineering para criar o template do prompt para o serviço e gerenciar a conversa.\n",
        "- Utilizem como base e referência todos os materiais apresentados, tanto de Generative AI quanto de Database for GenAI.\n",
        "  - Estruturem bem o trabalho, organizem em funções, expliquem e documentem bem os códigos e decisões.\n",
        "- Fiquem a vontade de complementar o contexto e o prompt para otimizar o serviço.\n",
        "- Fiquem a vontade de trazer técnicas que façam sentido e complemente o trabalho.\n",
        "\n",
        "### Dicas:\n",
        "- Comecem os desenvolvimentos de forma simples, testando os componentes e vá aumentando a complexidade gradativamente.\n",
        "- Utilizem os modelos LLMs da OpenAI ou da Azure com OpenAI.\n",
        "\n",
        "###**Orientações:**\n",
        "\n",
        "---\n",
        "####**Usem o Google Colab com Python e esse template para desenvolverem o trabalho.**\n",
        "---\n",
        "\n",
        "**1. Desenvolvimento e testes**, nessa parte é onde vocês podem explorar o desenvolvimento do trabalho aplicando as técnicas, testando as ferramentas e serviços de GenAI.\n",
        "  - Explorem diferentes formas de tratar o problema. Comecem testando os componentes e definindo a melhor estratégia.\n",
        "  - Testem as ferramentas, framework, APIs e técnicas de prompt engineering.\n",
        "  - Fiquem à vontade para explorar os serviços e frameworks vistos em aula: API da OpenAI Platform, API da Azure AI Foundry, LangChain e outros.\n",
        "  - Sejam criativos, mas não precisa de muita complexidade e podem explorar outras formas de desenvolver.\n",
        "  - Expliquem as decisões e racional do desenvolvimento. **Abuse dos comentários**.\n",
        "\n",
        "**2. Processo final**, aqui nessa parte separem apenas o processo final com um pipeline completo para o ChatBot, desde a instalação das bibliotecas até a simulação.\n",
        "  - Organizem em funções que façam sentido.\n",
        "  - Resultado esperado: Um processo estruturado utilizando LangChain e ChromaDB, criando um VectorDB com uma boa estratégia de indexação e busca (retriever), uma simulação do ChatBot (pode ser estruturando uma API ou alguma lógica para simular uma conversa).\n",
        "  - Exemplos: Deixem e/ou compartilhem no notebook exemplos de conversas.\n",
        "  - Testem bem esse pipeline antes, pois o professor tentará executar o processo para validar a implementação.\n",
        "\n",
        "###**Avaliação:**\n",
        "O trabalho será avaliado pelas seguintes diretrizes:\n",
        "  - Demonstração de conhecimento com os temas abordados em sala de aula.\n",
        "  - Utilização correta dos frameworks, APIs e aplicação das técnicas de prompt engineering.\n",
        "  - Organização, comentários e explicação certamente vão ajudar na nota.\n",
        "  - Resultado esperado seguindo as orientações do professor nesse template.\n",
        "\n",
        "###**Atenção:**\n",
        "- Usem a conta da **Azure AI Foundry** ou da **OpenAI Platform** para desenvolverem o trabalho, mas dê preferência para a conta da Azure por causa dos limites de crédito. **Não deixem suas credenciais no trabalho, por favor!**\n",
        "- Trabalhos iguais são passíveis de reprovação ou desconto de nota.\n",
        "- Respeitem a estrutura do template fornecido pelo professor.\n",
        "- Limite de 2 a 4 pessoas por grupo, de preferência o mesmo grupo do Startup One."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EISVWz-KmH5D"
      },
      "source": [
        "##**1. Desenvolvimento e testes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh9AhDTBmH5D"
      },
      "source": [
        "Desenvolva aqui:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTukXNWimOuU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "KEY_NGROK = \"\" # @param {\"type\":\"string\"}\n",
        "\n",
        "os.environ[\"KEY_NGROK\"] = KEY_NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, time, psutil, socket, subprocess, shutil\n",
        "\n",
        "CHROMA_HOST = \"127.0.0.1\" \n",
        "CHROMA_PORT = 8000\n",
        "\n",
        "def is_port_in_use(port, host=\"127.0.0.1\"):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(0.5)\n",
        "        return s.connect_ex((host, port)) == 0\n",
        "\n",
        "def wait_for_port(port, host=\"127.0.0.1\", timeout=30):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        if is_port_in_use(port, host):\n",
        "            return True\n",
        "        time.sleep(0.5)\n",
        "    return False\n",
        "\n",
        "def build_chroma_cmd(host, port):\n",
        "        chroma_exe = os.path.join(os.path.dirname(sys.executable), \"Scripts\", \"chroma.exe\")\n",
        "        return [chroma_exe, \"run\", \"--host\", host, \"--port\", str(port)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "chroma_exe = os.path.join(os.path.dirname(sys.executable), \"Scripts\", \"chroma.exe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\JOAO PC\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Scripts\\\\chroma.exe'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chroma_exe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.exists(chroma_exe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_port_in_use(CHROMA_PORT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chroma = build_chroma_cmd(CHROMA_HOST, CHROMA_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['c:\\\\Users\\\\JOAO PC\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Scripts\\\\chroma.exe',\n",
              " 'run',\n",
              " '--host',\n",
              " '127.0.0.1',\n",
              " '--port',\n",
              " '8000']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['c:\\\\Users\\\\JOAO PC\\\\AppData\\\\Local\\\\Program...>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subprocess.Popen(chroma, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wait_for_port(CHROMA_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando status do servidor...\n",
            "{\"nanosecond heartbeat\":1761933964922155600}\n",
            "\n",
            "Se a resposta acima for '{\"nanosecond heartbeat\":...}', o servidor está no ar!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100    44  100    44    0     0  29810      0 --:--:-- --:--:-- --:--:-- 44000\n"
          ]
        }
      ],
      "source": [
        "# Verifica se o servidor está respondendo (opcional, mas recomendado)\n",
        "print(\"Verificando status do servidor...\")\n",
        "!curl http://127.0.0.1:8000/api/v2/heartbeat\n",
        "print(\"\\nSe a resposta acima for '{\\\"nanosecond heartbeat\\\":...}', o servidor está no ar!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configurando e iniciando o Ngrok\n",
            "Servidor Chroma está online e acessível na URL: NgrokTunnel: \"https://operculate-vernon-unmissed.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "# Associar o Servidor Chroma com o Ngrok\n",
        "import time\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Configurando e iniciando o Ngrok\n",
        "print(\"Configurando e iniciando o Ngrok\")\n",
        "ngrok.set_auth_token(KEY_NGROK)\n",
        "#conf.get_default().auth_token = KEY_NGROK\n",
        "\n",
        "# Mata qualquer túnel anterior para garantir um início limpo\n",
        "try:\n",
        "  ngrok.kill()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Expor o servidor com Ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Servidor Chroma está online e acessível na URL: {public_url}\")\n",
        "# Inicia o túnel na porta 8000, onde nosso servidor Chroma está escutando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://operculate-vernon-unmissed.ngrok-free.dev'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "public_url.public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"nanosecond heartbeat\":1761933402510016300}\n",
            "\n",
            "Se a resposta acima for '{\"nanosecond heartbeat\":...}', o servidor está no ar! 🚀\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0    44    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100    44  100    44    0     0     62      0 --:--:-- --:--:-- --:--:--    62\n"
          ]
        }
      ],
      "source": [
        "url = public_url.public_url  \n",
        "\n",
        "# testa o endpoint de heartbeat\n",
        "!curl {url}/api/v2/heartbeat\n",
        "print(\"\\nSe a resposta acima for '{\\\"nanosecond heartbeat\\\":...}', o servidor está no ar! 🚀\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4NwScOmmg1c"
      },
      "source": [
        "##**2. Processo final**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "## LangChain\n",
        "!pip install langchain==0.3.27 langchain_community==0.3.27 langchain-openai==0.3.27 --quiet\n",
        "\n",
        "## ChromaDB\n",
        "!pip install langchain-chroma==0.2.5 chromadb-client==1.0.20 --quiet\n",
        "\n",
        "## Ngrok\n",
        "#!pip install pyngrok==7.3.0 --quiet\n",
        "\n",
        "## Outras\n",
        "!pip install fastapi==0.116.1 uvicorn==0.35.0 nest_asyncio==1.6.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bibliotecas e configuração do ambiente\n",
        "import os\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import HTMLResponse\n",
        "from pydantic import BaseModel\n",
        "import uuid\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import chromadb\n",
        "\n",
        "# Bibliotecas LanChain\n",
        "import langchain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Ativa o modo debug do LangChain (para ver o que acontece por baixo do capô!)\n",
        "flg_log_debug = False\n",
        "langchain.debug = flg_log_debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#**Quem Somos?**\n",
            "Bem-vindo à AutoToya Veículos, sua concessionária oficial da Toyota!\n",
            "\n",
            "Desde o início, nosso compromisso é oferecer a você uma experiência completa e personalizada, indo além da venda de veículos. Na AutoToya, você encontrará uma ampla variedade de modelos Toyota, desde os mais recentes lançamentos até veículos seminovos de alta qualidade.\n",
            "\n",
            "Na AutoToya, estamos comprometidos em oferecer a você uma experiência completa e personalizada. Contamos com uma ampla variedade de modelos T\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1xZIpwP1JggimbZrtGLnuaTf6DrhbAS9T/view?usp=sharing\"\n",
        "\n",
        "file_id = url.split(\"/d/\")[1].split(\"/\")[0]\n",
        "url_download = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "file_name = \"contexto_rag_chatbot.txt\"\n",
        "\n",
        "response = requests.get(url_download)\n",
        "response.raise_for_status()\n",
        "\n",
        "with open(file_name, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "arquivo_contexto = file_name\n",
        "with open(arquivo_contexto, 'r', encoding='utf-8') as file:\n",
        "    contexto = file.read()\n",
        "\n",
        "print(contexto[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O contexto foi dividido em 12 documentos (chunks).\n"
          ]
        }
      ],
      "source": [
        "## Por quantidade de tokens\n",
        "import tiktoken\n",
        "\n",
        "# Escolhe o tokenizer utilizando o mesmo método do modelo de Embeddings\n",
        "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
        "\n",
        "# Função que conta tokens\n",
        "def fn_conta_token(text: str) -> int:\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Configura o splitter medindo por tokens\n",
        "text_splitter_token = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,         # máximo de tokens por chunk\n",
        "    chunk_overlap = 50,       # sobreposição de tokens por chunk\n",
        "    length_function = fn_conta_token\n",
        ")\n",
        "\n",
        "# O método create_documents() divide o contexto nos chunks e já o formata em um objeto tipo Document do LangChain.\n",
        "documents = text_splitter_token.create_documents([contexto])\n",
        "print(f\"O contexto foi dividido em {len(documents)} documentos (chunks).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JOAO PC\\AppData\\Local\\Temp\\ipykernel_4580\\114160704.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding_model = OpenAIEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "# Define qual o modelo de embeddings, no caso vamos usar um modelo da OpenAI - text-embedding-3-small\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    openai_api_key = OPENAI_API_KEY,\n",
        "    model = 'text-embedding-3-small'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "client_chromadb = chromadb.HttpClient(\n",
        "    host = CHROMA_HOST\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# O nome da \"coleção\" no servidor. Pense nisso como o nome de uma tabela em um banco de dados.\n",
        "CHROMADB_COLLECTION_NAME = \"chromadb_vactorstore_autotoya\"\n",
        "\n",
        "# Cria o Vectorstore com o Chroma a partir dos documentos divididos\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents = documents,\n",
        "    embedding = embedding_model,\n",
        "    client = client_chromadb,  # Passamos o \"client\" em vez do persist_directory\n",
        "    collection_name = CHROMADB_COLLECTION_NAME\n",
        ")\n",
        "\n",
        "# Instancia o retriever (podemos controlar o número de chunks do retorno da análise de similaridade)\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type = \"similarity\",   # similarity por \"mmr\" - (max marginal relevance) - pode trazer mais diversidade nos resultados\n",
        "    search_kwargs = {\"k\": 3}      # define quantos chunks vão retornar\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#**Quem Somos?**\n",
            "Bem-vindo à AutoToya Veículos, sua concessionária oficial da Toyota!\n",
            "\n",
            "Desde o início, nosso compromisso é oferecer a você uma experiência completa e personalizada, indo além da venda de veículos. Na AutoToya, você encontrará uma ampla variedade de modelos Toyota, desde os mais recentes lançamentos até veículos seminovos de alta qualidade.\n",
            "\n",
            "Na AutoToya, estamos comprometidos em oferecer a você uma experiência completa e personalizada. Contamos com uma ampla variedade de modelos Toyota, desde os mais recentes lançamentos até veículos seminovos de alta qualidade, além de preços exclusivos para taxistas, pessoas com deficiência (PCD), produtores rurais e vendas diretas para CNPJ.\n",
            "\n",
            "Também cuidamos do seu Toyota como ninguém. Oferecemos peças originais e serviços especializados para garantir a máxima segurança, desempenho e durabilidade do seu veículo.\n",
            "\n",
            "Na AutoToya, acreditamos que escolher um carro é mais do que uma compra, é um momento especial. Venha nos visitar, conheça nossa linha completa e encontre o Toyota perfeito para você!\n",
            "\n",
            "Aqui todos os veículos seminovos são revisados e possuem garantia de um ano pela concessionária (motor e câmbio).\n",
            "\n",
            "## Serviços e Benefícios\n",
            "- **Consórcio AutoToya:** Facilite a aquisição de carros, motos ou utilitários, novos ou seminovos, com planos sob medida para você.\n",
            "- **Peças Originais Toyota:** Garanta segurança, durabilidade e desempenho com peças genuínas.\n",
            "- **Test-Drive Disponível:** Experimente o Toyota dos seus sonhos!\n",
            "- **Garantia:** Garantia para todos os veículos e serviços.\n",
            "\n",
            "## Experimente e Conheça Mais\n",
            "Venha nos visitar e aproveite para realizar um test-drive em nossos veículos.\n",
            "Nossa equipe está pronta para atender você com excelência e encontrar o Toyota ideal para suas necessidades.\n",
            "\n",
            "Saiba mais sobre nossos modelos, serviços e ofertas em nosso site: www.autotoyaveiculos.com.br.\n",
            "Saiba mais sobre nossos modelos, serviços e ofertas em nosso site: www.autotoyaveiculos.com.br.\n",
            "\n",
            "Nos destacamos com vantagens competitivas dos principais modelos da Toyota no mercado brasileiro, tornando-os escolhas populares entre os consumidores.\n",
            "\n",
            "AutoToya Toyota – Mais do que vender carros, realizamos sonhos.\n",
            "\n",
            "\n",
            "#**Sobre a concessionária AutoToya Veículos:**\n",
            "\n",
            "## Horário de funcionamento:\n",
            "- Segunda a Sexta: 8h às 18h\n",
            "- Sábado: 9h às 14h\n",
            "- Domingo: fechado\n",
            "\n",
            "## Nossas Unidades\n",
            "Visite uma de nossas lojas e descubra o Toyota perfeito para você:\n",
            "- Osasco: Avenida dos Autonomistas, 890 - Centro, Osasco - SP, 06020-010\n",
            "- São Paulo: Avenida Paulista, 1106 - Bela Vista, São Paulo - SP, 01311-000\n",
            "- Curitiba: Avenida Sete de Setembro, 2775 - Rebouças, Curitiba - PR, 80230-010\n",
            "\n",
            "## Informações de contato:\n",
            "- Telefone: (11) 4002-8922\n",
            "- WhatsApp: (11) 98888-7777\n",
            "- Site: www.autotoyaveiculos.com.br\n",
            "- Instagram: contato@iautoveiculos.com.br\n",
            "\n",
            "\n",
            "#**Sobre os veículos disponíveis:**\n"
          ]
        }
      ],
      "source": [
        "# Teste da busca por similaridade\n",
        "query = \"O que é a AutoToya?\"\n",
        "\n",
        "docs_retornados = vectorstore.similarity_search(query, k=2)\n",
        "print(docs_retornados[0].page_content)\n",
        "print(docs_retornados[1].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configura o chat prompt template com System Rules - regras fixas do assistente\n",
        "system_template = \"\"\"\n",
        "Você é um assistente virtual altamente inteligente e atende clientes de uma loja (concessionária) de venda de veículos chamada AutoToya Veículos.\n",
        "\n",
        "#**Seu papel e objetivo é:**\n",
        "1. Responder dúvidas sobre a loja, veículos e serviços oferecidos pela loja.\n",
        "2. Qualificar e engajar o cliente para aumentar o interesse nos veículos (priorize os veículos zero km, mas não perca a venda.)\n",
        "3. Convidar o cliente para fazer um test-drive e conhecer o veículo pessoalmente.\n",
        "4. Ajudar o cliente a agendar a visita de test-drive.\n",
        "\n",
        "##**Orientações e regras que deve seguir:**\n",
        "- Seu nome é ToyaBot.\n",
        "- Responda sempre em Português (pt-br)\n",
        "- Seja simpático e prestativo. Use emojs para deixar a conversa divertida, como: 🤖 🚗 👊 💪 🚀 😊 🪄\n",
        "- Responda sempre de forma educada e clara.\n",
        "- Nunca destratar um cliente sendo rude ou arrogante por exemplo.\n",
        "- Use o contexto fornecido para responder à pergunta de forma coerente.\n",
        "- **Mantenha uma coerência nas respostas com base no histórico do chat.**\n",
        "- Se o contexto não for suficiente, peça mais detalhes para o cliente.\n",
        "- Responda **APENAS** perguntas que estão no contexto, se não souber diga que não pode responder (respnda de forma cordial) e interaja voltando para o contexto da conversa e seu objetivo.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_template),   ## regras fixas do assistente\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\", n_messages = 6, optional = True), ## Insere o histórico de conversas aqui (opcional e limitado a N mensagens) / Monta uma lista de mensagens (AIMessage, HumanMessage, SystemMessage)\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")        ## define a entrada do usuário - entrada dinâmica\n",
        "])\n",
        "# optional = True, deixa o histórico como opcional caso esteja vazio para evitar erros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JOAO PC\\AppData\\Local\\Temp\\ipykernel_4580\\155446263.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n"
          ]
        }
      ],
      "source": [
        "# Modelo LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.5,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JOAO PC\\AppData\\Local\\Temp\\ipykernel_4580\\2282799827.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory_test = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "# Configura o memory para manter o histórico da conversa\n",
        "memory_test = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        "    )\n",
        "\n",
        "# Cria a cadeia de conversa com retriever e memória\n",
        "chain_test = ConversationalRetrievalChain.from_llm(\n",
        "    llm = llm,\n",
        "    retriever = retriever,\n",
        "    memory = memory_test,\n",
        "    combine_docs_chain_kwargs = {\"prompt\": chat_prompt},\n",
        "    chain_type = \"stuff\", # O tipo \"stuff\" simplesmente junta os chunks em um único prompt.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função com o loop de iteração simulando um chat\n",
        "def fn_chat_test():\n",
        "  \"\"\"Função para simular um chat com o assistente\"\"\"\n",
        "  print(\"Iniciado o Chatbot da AutoToya! Digite 'sair' para encerrar.\\n\")\n",
        "\n",
        "  # Pré-carregar uma mensagem de histórico com uma saudação inicial\n",
        "  memory_test.chat_memory.clear() ## limpa o histórico (buffer)\n",
        "  memory_test.chat_memory.add_ai_message(\"Olá! Seja bem-vindo. Sou um assistente virtual, me chamo ToyaBot 🤖. \\nComo posso ajudar você hoje? Está em busca de informações sobre algum veículo ou serviço da AutoToya Veículos? 🚗✨\")\n",
        "  print(f\"ToyaBot 🤖: {memory_test.chat_memory.messages[0].content}\")\n",
        "\n",
        "  while True:\n",
        "    user_question = input(\"\\nVocê: \")\n",
        "    if user_question.lower() == \"sair\":\n",
        "        print(\"ToyaBot 🤖: 👋 Conversa encerrada. Até logo! 🚗\")\n",
        "        break\n",
        "    response = chain_test.invoke({\"question\": user_question})\n",
        "    #response = chain_test.run(question = user_question) ## Essa função está depreciada\n",
        "    print(f\"ToyaBot 🤖: {response['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciado o Chatbot da AutoToya! Digite 'sair' para encerrar.\n",
            "\n",
            "ToyaBot 🤖: Olá! Seja bem-vindo. Sou um assistente virtual, me chamo ToyaBot 🤖. \n",
            "Como posso ajudar você hoje? Está em busca de informações sobre algum veículo ou serviço da AutoToya Veículos? 🚗✨\n",
            "ToyaBot 🤖: A AutoToya Veículos é a sua concessionária oficial da Toyota! 🚗 Desde o início, nosso compromisso é oferecer uma experiência completa e personalizada, indo além da venda de veículos. Na AutoToya, você encontrará uma ampla variedade de modelos Toyota, desde os mais recentes lançamentos até veículos seminovos de alta qualidade.\n",
            "\n",
            "Estamos dedicados a cuidar do seu Toyota, oferecendo peças originais e serviços especializados para garantir a segurança, desempenho e durabilidade do seu veículo. Além disso, contamos com preços exclusivos para taxistas, pessoas com deficiência (PCD), produtores rurais e vendas diretas para CNPJ.\n",
            "\n",
            "Na AutoToya, acreditamos que escolher um carro é mais do que uma compra, é um momento especial. Venha nos visitar e descubra o Toyota perfeito para você! 😊\n",
            "ToyaBot 🤖: 👋 Conversa encerrada. Até logo! 🚗\n"
          ]
        }
      ],
      "source": [
        "flg_log_debug = False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    langchain.debug = flg_log_debug\n",
        "    fn_chat_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerenciamento de múltiplos usuários (memória por sessão)\n",
        "sessions = {}\n",
        "\n",
        "def get_chat_chain(session_id):\n",
        "    if session_id not in sessions:\n",
        "\n",
        "        # Configura o memory para manter o histórico da conversa\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "            )\n",
        "\n",
        "        # Cria a cadeia de conversa com retriever e memória\n",
        "        chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm = llm,\n",
        "            retriever = retriever,\n",
        "            memory = memory,\n",
        "            combine_docs_chain_kwargs = {\"prompt\": chat_prompt},\n",
        "            chain_type = \"stuff\", # O tipo \"stuff\" simplesmente junta os chunks em um único prompt.\n",
        "            )\n",
        "\n",
        "        # Carrega mensagem inicial\n",
        "        memory.chat_memory.add_ai_message(\"Olá! Seja bem-vindo. Sou um assistente virtual, me chamo ToyaBot 🤖. \\nComo posso ajudar você hoje? Está em busca de informações sobre algum veículo ou serviço da AutoToya Veículos? 🚗✨\")\n",
        "\n",
        "        # Gerencia sessão - cada chat vira um objeto chain identificado pelo session_id\n",
        "        sessions[session_id] = chain\n",
        "    return sessions[session_id]\n",
        "\n",
        "# Configura o FastAPI para criar a estrutura de API\n",
        "app = FastAPI(title=\"API - Chatbot ToyaBot\", version=\"1.0\")\n",
        "\n",
        "# Estrutura resposta com parse do pydantic\n",
        "class ChatRequest(BaseModel):\n",
        "    session_id: str\n",
        "    question: str\n",
        "\n",
        "# Rota para chamar a função que chama o LLM e toda estrutura do chat (chain, retriever, vectorstore, etc.)\n",
        "@app.post(\"/chat\")\n",
        "def chat(req: ChatRequest):\n",
        "    chain = get_chat_chain(req.session_id)\n",
        "    #resposta = chain.run(question = req.question)\n",
        "    resposta = chain.invoke({\"question\": req.question})\n",
        "    resposta = resposta['answer']\n",
        "    return {\"answer\": resposta}\n",
        "\n",
        "# Rota para inicializar o chat com a mensagem de saudação inicial\n",
        "@app.get(\"/start/{session_id}\")\n",
        "def start(session_id: str):\n",
        "    chain = get_chat_chain(session_id)\n",
        "    msg_inicial = chain.memory.chat_memory.messages[0].content\n",
        "    return {\"answer\": msg_inicial}\n",
        "\n",
        "# Rota principal que renderiza página HTML do chat (simples)\n",
        "@app.get(\"/\", response_class = HTMLResponse)\n",
        "def index():\n",
        "    return \"\"\"\n",
        "<html>\n",
        "<head><title>🤖 ToyaBot Chat 🤖</title></head>\n",
        "<body style=\"font-family:Arial\">\n",
        "  <h2>🤖 ToyaBot - AutoToya Veículos 🤖</h2>\n",
        "  <div id=\"chat\" style=\"border:1px solid #ccc; padding:10px; width:400px; height:300px; overflow-y:scroll;\"></div>\n",
        "  <input type=\"text\" id=\"msg\" style=\"width:300px;\" placeholder=\"Digite sua mensagem...\"/>\n",
        "  <button onclick=\"send()\">Enviar</button>\n",
        "  <script>\n",
        "    let session_id = Math.random().toString(36).substring(7);\n",
        "\n",
        "    async function startChat(){\n",
        "      let r = await fetch('/start/' + session_id);\n",
        "      let data = await r.json();\n",
        "      document.getElementById(\"chat\").innerHTML += \"<b>ToyaBot:</b> \" + data.answer + \"<br/>\";\n",
        "    }\n",
        "\n",
        "    async function send(){\n",
        "      let msg = document.getElementById(\"msg\").value;\n",
        "      document.getElementById(\"chat\").innerHTML += \"<b>Você:</b> \" + msg + \"<br/>\";\n",
        "      let r = await fetch('/chat', {\n",
        "        method: 'POST',\n",
        "        headers: {'Content-Type':'application/json'},\n",
        "        body: JSON.stringify({session_id: session_id, question: msg})\n",
        "      });\n",
        "      let data = await r.json();\n",
        "      document.getElementById(\"chat\").innerHTML += \"<b>ToyaBot:</b> \" + data.answer + \"<br/>\";\n",
        "      document.getElementById(\"msg\").value=\"\";\n",
        "    }\n",
        "\n",
        "    // inicia o chat chamando o backend\n",
        "    startChat();\n",
        "  </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [4580]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL pública gerada pelo Ngrok: NgrokTunnel: \"https://operculate-vernon-unmissed.ngrok-free.dev\" -> \"http://localhost:8001\"\n",
            "INFO:     187.255.127.46:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     187.255.127.46:0 - \"GET /start/0lkk0q HTTP/1.1\" 200 OK\n",
            "INFO:     187.255.127.46:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     187.255.127.46:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     187.255.127.46:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [4580]\n"
          ]
        }
      ],
      "source": [
        "# Executa o App no Colab + ngrok\n",
        "if __name__ == \"__main__\":\n",
        "  #ngrok.set_auth_token(KEY_NGROK) ## Já fizemos essa configuração antes (agora é opcional)\n",
        "  public_url = ngrok.connect(8001)\n",
        "  print(\"URL pública gerada pelo Ngrok:\", public_url)\n",
        "\n",
        "  nest_asyncio.apply()\n",
        "  uvicorn.run(app, host=\"0.0.0.0\", port=8001, reload=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finaliza sessão do Ngrok (Agente)\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9FHz-nrmg1d"
      },
      "source": [
        "Desenvolva aqui:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz_9WPHVmkvP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMnpzLAbmkst"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmdi4LFrmkhz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4kHa8kfmkfX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
